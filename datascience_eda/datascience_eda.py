import pandas as pd

# region import libraries for clustering
from sklearn import cluster, datasets, metrics
from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans
from sklearn.datasets import make_blobs
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.preprocessing import StandardScaler

from yellowbrick.cluster import SilhouetteVisualizer

from scipy.cluster.hierarchy import (
    average,
    complete,
    dendrogram,
    fcluster,
    single,
    ward,
)

# endregion

# import libraries for eda of text features
import seaborn as sns
import matplotlib.pyplot as plt 
import nltk
import spacy
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.corpus import stopwords
from IPython.display import Markdown, display
from wordcloud import WordCloud
# from wordcloud import WordCloud, STOPWORDS

# endregion


def get_clustering_default_hyperparameters():
    """create a dictionary listing default hyperparameters for K-Means and DBSCAN clustering

    Returns
    -------
    dict
        a dictionary with key = clustering algorithm name, value = a dictionary of hyperameters' name and value

    Examples
    -------
    >>> hyper_dict = get_clustering_default_hyperparameters()
    >>> hyper_dict["distance_metric"] = "cosine"
    >>> hyper_dict["K-Means"]["n_clusters"] = (1, 10)
    >>> hyper_dict["DBSCAN"]["eps"] = [1]
    >>> hyper_dict["DBSCAN"]["min_samples"] = [3]
    >>> initial_clustering(X, hyperparameter_dict=hyper_dict)
    """
    clustering_default_hyperparameters = {
        "distance_metric": "euclidean",
        "K-Means": {"n_clusters": (3, 10)},
        "DBSCAN": {"eps": (1, 3), "min_samples": (3, 10)},
    }
    return clustering_default_hyperparameters


def explore_clustering(
    df, hyperparameter_dict=get_clustering_default_hyperparameters()
):
    """fit and plot K-Means, DBScan clustering algorithm on the dataset

    Parameters
    ----------
    df : pandas.DataFrame
        the dataset (X)
    hyperparameter_dict : dict, optional
        the distance metric and hyperparameters to be used in the clustering algorithm, by default get_clustering_default_hyperparameters()

    Returns
    -------
    dict
        a dictionary with each key = a clustering model name, value = list of plots generated by that model

    Examples
    -------
    >>> explore_clustering(X)
    """
    result = {}  # a dictionary to store charts generated by clustering models

    # ---clustering with K-Means-------
    # get n_clusters from hyperparameter_dict

    # visualize using KElbowVisualizer

    # visualize using SilhouetteVisualizer

    # plot PCA clusters

    # add all plots to result dictionary

    # ---clustering with DBSCAN---------
    # get eps and min_samples from hyperparameter_dict["DBSCAN"]

    # visualize using SilhouetteVisualizer

    # plot PCA clusters

    # add all plots to result dictionary

    return result

def printmd(string):
    display(Markdown(string))

def explore_text_columns(df, text_col=[], params=dict()):
    """Performs EDA of text features.
    - prints the summary statistics of character length
    - plots the distribution of character length
    - prints the summary statistics of word count
    - plots the distribution of word count
    - plots the word cloud
    - plots bar chart of top n stopwords
    - plots bar chart of top n words other than stopwords
    - plots bar chart of top n bigrams
    - plots the distribution of polarity and subjectivity scores
    - plots bar charts of sentiments, name entities and part of speech tags


    Parameters
    ----------
    df : pandas.DataFrame
        the dataset (X)
    text_col : str
        name of text column
    params : dict
        a dictionary of parameters

    Returns
    -------
    list
        A list of plot objects created by this function

    Examples
    -------
    >>> eda_text_columns(X)
    """
    result = []

    # identify text columns if not specified by user
    if not text_col:
        non_num = df.columns[df.dtypes == ("object" or "string")]
        for col in non_num:
            if df[col].unique().shape[0]/df.shape[0] > 0.75:
                if df[col].str.split().apply(len).median() > 5:
                    text_col.append(col)

        if not text_col:
            raise Exception("Could not identify any text column. Please pass the text column(s) when calling the function")
        else:
            print("Identified the following as text columns:", text_col)
            result.append(text_col)

    # print average, minimum, maximum and median character length of text
    # show the shortest and longest text (number of characters)
    print("\n")
    for col in text_col:

        printmd("## Exploratory Data Analysis of \"" +col+ "\" column:<br>")
        
        printmd("### Character Length:<br>")

        mean_char_length = df[col].str.len().mean()
        median_char_length = df[col].str.len().median()
        longest_char_length = df[col].str.len().max()
        longest_text = df[col][df[col].str.len() == longest_char_length].unique()
        shortest_char_length = df[col].str.len().min()
        shortest_text = df[col][df[col].str.len() == shortest_char_length].unique()

        printmd(f"- The average character length of text is {mean_char_length:.2f}")
        printmd(f"- The median character length of text is {median_char_length:.0f}")

        printmd(f"- The longest text(s) has {longest_char_length:.0f} characters:\n")
        
        for text in longest_text:
            printmd("\"" + text + "\"")

        printmd(f"- The shortest text(s) has {shortest_char_length:.0f} characters:\n")
        
        for text in shortest_text:
            printmd("\"" + text + "\"<br><br>")
        
        result.append([round(mean_char_length, 2), median_char_length,
                    longest_char_length, longest_text[0],
                    shortest_char_length, shortest_text[0]])

    # plot a histogram of the length of text (number of characters)
        printmd(f"#### Histogram of number of characters in \"{col}\":")
        sns.set_theme(style="whitegrid")
        plt.rcParams.update({'figure.figsize': (12,8)})
#        plt.figure(1)
        char_length_plot = sns.histplot(data=df[col].str.len());
        plt.xlabel("Number of characters in "+'"'+col+'"');
        result.append(char_length_plot)
        plt.show()
        plt.close()

    # print average, minimum, maximum and median number of words
    # show text with most number of words
        printmd("### Word Count:<br>")

        mean_word_count = df[col].str.split().apply(len).mean()
        median_word_count = df[col].str.split().apply(len).median()
        highest_word_count = df[col].str.split().apply(len).max()
        text_most_words = df[col][df[col].str.split().apply(len) == highest_word_count].unique()

        printmd(f"- The average number of words in \"{col}\": {mean_word_count:.2f}")
        printmd(f"- The median number of words in \"{col}\": {median_word_count:.0f}")

        printmd(f"- The text(s) in \"{col}\" with most words ({highest_word_count:.0f} words):\n")

        for text in text_most_words:
            printmd("\"" + text + "\"")

        result.append([round(mean_word_count, 2), median_word_count,
                    highest_word_count, text_most_words])

    # plot a histogram of the number of words
        printmd(f"#### Histogram of number of words in \"{col}\":")
#        plt.figure(2)
        word_count_plot = sns.histplot(data=df[col].str.split().apply(len));
        plt.xlabel("Number of words in "+'"'+col+'"');
        result.append(word_count_plot)
        plt.show();
        plt.close()
        printmd("<br>")

    # plot word cloud of text
        printmd("### Word Cloud:<br>")
        wordcloud = WordCloud(random_state=1).generate(' '.join(df[col]))
#        plt.figure(3)
        plt.rcParams.update({'figure.figsize': (12,8)})
        wordcloud_plot = plt.imshow(wordcloud, interpolation="bilinear")
        plt.axis("off")
        result.append(wordcloud_plot)
        plt.show();
        plt.close()        
        
        printmd("<br>")

    # plot a bar chart of the top stopwords
        stop=set(stopwords.words('english'))
        all_words=df[col].str.split()
        all_words=all_words.values.tolist()
        corpus=[word for i in all_words for word in i]
        corpus=(pd.DataFrame(pd.DataFrame(corpus, columns=["counts"])
                            .counts.value_counts())
                            .reset_index())
        corpus.columns=['words', 'counts']
        
        all_stopwords = corpus.merge(pd.DataFrame(stop, columns=['words']), on="words", how="right")
        
        printmd("### Bar Chart of the top stopwords:<br>")
        stopwords_plot = sns.barplot(y="words", x="counts", data=all_stopwords.sort_values(by="counts", ascending=False).head(10));
        plt.ylabel("Stop Words");
        plt.xlabel("Count");
        result.append(stopwords_plot)
        plt.show()

    # plot a bar chart of words other than stopwords

    # plot a bar chart of top bigrams

    # plot the distribution of polarity scores

    # plot the distribution of subjectivity scores

    # plot a bar chart of sentiments: Positive, Negative and Neutral

    # plot a bar chart of named entities

    # plot a bar chart of most common tokens per entity

    # plot a bar chart of Part-of-speech tags


    return result


def explore_numeric_columns(
    data, hist_cols=None, pairplot_cols=None, corr_method="pearson"
):
    """This function will create common exploratory analysis visualizations on numeric columns in the dataset which is provided to it.

    The visualizations that will be created are:

    1. Histograms for all numeric columns or for columns specified in optional paramter `hist_cols`
    2. Scatterplot Matrix (SPLOM) for all numeric columns or for columns specified in optional paramter `hist_cols`
    3. Heatmap showing correlation coefficient (pearson, kendall or spearman) between all numeric columns

    Parameters
    ----------
    data : pandas.DataFrame
        The dataframe for which exploratory analysis is to be carried out
    hist_cols : list, optional
        If passed, it will limit histograms to a subset of columns
    pairplot_cols : list, optional
        If passed, it will limit pairplots to a subset of columns
    corr_method : str, optional
        Chooses the metric for correlation. Default value is 'pearson'. Possible values:
            * pearson : standard correlation coefficient
            * kendall : Kendall Tau correlation coefficient
            * spearman : Spearman rank correlation

    Returns
    -------
    list
        A list of plot objects created by this function

    Examples
    -------
    >>> explore_numeric_columns(df)
    """

    # Generate plots

    results = (
        []
    )  # List will store plot objects created by this function to return to user
    return results


def explore_categorical_columns(df, categorical_cols):
    """Performs EDA of categorical features.
    - Creates a dataframe containing column names and corresponding details about unique values, null values and most frequent category in the column
    - Plots countplots for given categorical columns

    Parameters
    ----------
    df : pandas.DataFrame
        the dataset (X)
    categorical_col : list
        name of categorical column(s)
        
    Returns
    -------
    dataframe
        A Dataframe with column names, corresponding unique categories, count of null values, percentage of null values and most frequent categories    
        
    Examples
    -------
    >>> explore_categorical_columns(X, ['col1', 'col2'])
    """

    # Create dataframe with column names, unique categories, number of nulls, percentage of nulls and most frequent categories
    
    # Sort the dataframe using percentage of nulls (descending)  

    # plot countplots of provided categorical features

    return cat_df 
